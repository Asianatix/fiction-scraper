#!/usr/bin/env python3

import argparse
import logging
import re
import subprocess
import sys
from urllib.parse import urlparse

from fiction_scraper import spiders


# TODO remove
import requests
import requests_cache
requests_cache.install_cache('cache')

DEFAULT_PANDOC_ARGS = (
    'pandoc',
    '--from', 'html+raw_html',
    '--standalone',
    '--epub-chapter-level', '3')


parser = argparse.ArgumentParser(
    description='A variety of scrapers for online fiction and fanfiction',
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog='\n'.join([
        'Suppported sites and sample URLs:',
        '',
        *(f'  {s.name:21s} {s.url}' for s in spiders.values()),
        '',
        'Note, some sites may have multiple stories at additional URLs.'
    ]))
parser.add_argument('URL',
    help='URL of story to download')
parser.add_argument('-o', '--output',
    default=None,
    help="""Output file for the story. Unless Pandoc is disabled, the story
        will be postprocesed using Pandoc. Pandoc will autodetect the output
        file format from the file extension. If omitted, HTML will be written
        to standard output.""")
parser.add_argument('-p', '--no-pandoc',
    action='store_true',
    help="""Don't postprocess story with Pandoc, HTML output only""")
parser.add_argument('-v', '--verbose',
    help='Verbose output',
    action='store_true')
parser.add_argument('-d', '--debug',
    help='Debug output',
    action='store_true')
args = parser.parse_args()


if args.debug:
    logging.basicConfig(level=logging.DEBUG)
elif args.verbose:
    logging.basicConfig(level=logging.INFO)


if not args.no_pandoc:
    if args.output:
        pandoc_args = [*DEFAULT_PANDOC_ARGS, '-o', args.output]
    else:
        pandoc_args = DEFAULT_PANDOC_ARGS

    pandoc = subprocess.Popen(pandoc_args,
        stdin=subprocess.PIPE,
        stderr=sys.stderr,
        stdout=sys.stdout)

    output = pandoc.stdin
elif args.output:
    output = open(args.output, 'wb')
else:
    output = sys.stdout.buffer


url = args.URL
domain = urlparse(url).netloc
domain = re.sub(r'^www\.', '', domain)
spider = spiders[domain]()
spider.crawl(url, output)
output.close()


if not args.no_pandoc:
    pandoc.wait()
